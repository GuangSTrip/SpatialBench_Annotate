import json
import os
from typing import List, Dict, Optional
from datetime import datetime

class DatasetManager:
    """æ•°æ®é›†ç®¡ç†å™¨ï¼Œè´Ÿè´£å¤„ç†æ•°æ®é›†ã€æ ·æœ¬å’Œç‰‡æ®µ"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.datasets = {}
        self.segments = {}
        self._load_datasets()
    
    def _load_datasets(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†"""
        # print(f"ğŸ” å¼€å§‹åŠ è½½æ•°æ®é›†ï¼Œæ•°æ®ç›®å½•: {self.data_dir}")
        
        if not os.path.exists(self.data_dir):
            # print(f"ğŸ“ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºç›®å½•: {self.data_dir}")
            os.makedirs(self.data_dir)
            self._create_sample_data()
            return
        
        # print(f"ğŸ“ æ•°æ®ç›®å½•å­˜åœ¨ï¼Œå¼€å§‹æ‰«ææ–‡ä»¶...")
        
        # åŠ è½½æ•°æ®é›†æ–‡ä»¶
        for filename in os.listdir(self.data_dir):
            # print(f"ğŸ“„ å‘ç°æ–‡ä»¶: {filename}")
            if filename.endswith('.json') and not filename.endswith('_segments.json'):
                # æ£€æŸ¥æ–‡ä»¶å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºæ•°æ®é›†æ–‡ä»¶
                filepath = os.path.join(self.data_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = json.load(f)
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°æ®é›†å¿…éœ€å­—æ®µ
                        if isinstance(content, dict) and 'id' in content and 'samples' in content:
                            dataset_id = filename.replace('.json', '')
                            self.datasets[dataset_id] = content
                            # print(f"âœ… åŠ è½½æ•°æ®é›†æ–‡ä»¶: {filename} -> {dataset_id}")
                        elif isinstance(content, list) and len(content) > 0:
                            # å¤„ç†EgoExo4Dæ ¼å¼çš„æ•°æ®
                            dataset_id = filename.replace('.json', '')
                            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                            converted_data = self._convert_egoexo4d_format(content, dataset_id)
                            self.datasets[dataset_id] = converted_data
                            # print(f"âœ… è½¬æ¢å¹¶åŠ è½½EgoExo4Dæ•°æ®é›†: {filename} -> {dataset_id}")
                        else:
                            print(f"âš ï¸ è·³è¿‡éæ•°æ®é›†æ–‡ä»¶: {filename}")
                except Exception as e:
                    print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥ {filename}: {e}")
        
        # åŠ è½½ç‰‡æ®µæ–‡ä»¶
        for filename in os.listdir(self.data_dir):
            if filename.endswith('.json') and 'segments' in filename:
                dataset_id = filename.replace('_segments.json', '')
                filepath = os.path.join(self.data_dir, filename)
                # print(f"âœ… åŠ è½½ç‰‡æ®µæ–‡ä»¶: {filename} -> {dataset_id}")
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        self.segments[dataset_id] = json.load(f)
                        # print(f"âœ… æˆåŠŸåŠ è½½ç‰‡æ®µ: {dataset_id}")
                except Exception as e:
                    print(f"âŒ åŠ è½½ç‰‡æ®µå¤±è´¥ {dataset_id}: {e}")
        
        print(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆ: {len(self.datasets)} ä¸ªæ•°æ®é›†, {len(self.segments)} ä¸ªç‰‡æ®µæ–‡ä»¶")
        
        # ä¸ºæ‰€æœ‰æ•°æ®é›†ç¡®ä¿æœ‰segmentæ–‡ä»¶
        for dataset_id in self.datasets.keys():
            if dataset_id not in self.segments:
                self.segments[dataset_id] = {'segments': []}
                # åˆ›å»ºç©ºçš„segmentæ–‡ä»¶
                filepath = os.path.join(self.data_dir, f"{dataset_id}_segments.json")
                try:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump({'segments': []}, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ“ ä¸ºæ•°æ®é›† {dataset_id} åˆ›å»ºç©ºçš„segmentæ–‡ä»¶")
                except Exception as e:
                    print(f"âš ï¸ åˆ›å»ºsegmentæ–‡ä»¶å¤±è´¥ {dataset_id}: {e}")
        
        # print(f"ğŸ“‹ æ•°æ®é›†IDåˆ—è¡¨: {list(self.datasets.keys())}")
    
    def _convert_egoexo4d_format(self, egoexo4d_data: List[Dict], dataset_id: str) -> Dict:
        """å°†EgoExo4Dæ ¼å¼è½¬æ¢ä¸ºæ ‡å‡†æ•°æ®é›†æ ¼å¼"""
        # print(f"ğŸ”„ å¼€å§‹è½¬æ¢EgoExo4Dæ ¼å¼æ•°æ®...")
        
        # åˆ›å»ºæ ‡å‡†æ•°æ®é›†ç»“æ„
        standard_dataset = {
            "id": dataset_id,
            "name": f"EgoExo4D Dataset ({dataset_id})",
            "description": "Converted from EgoExo4D format",
            "created_at": datetime.now().isoformat(),
            "samples": []
        }
        
        # è½¬æ¢æ¯ä¸ªæ ·æœ¬
        for i, take in enumerate(egoexo4d_data):
            if 'take_name' in take and 'frame_aligned_videos' in take:
                # ç”Ÿæˆæ ·æœ¬ID
                sample_id = self._generate_egoexo4d_sample_id(take['take_name'])
                
                # æ„å»ºè§†é¢‘è·¯å¾„
                video_paths = []
                if 'frame_aligned_videos' in take and isinstance(take['frame_aligned_videos'], dict):
                    for camera_name, video_path in take['frame_aligned_videos'].items():
                        # è½¬æ¢ä¸ºæœ¬åœ°é™æ€è·¯å¾„æ ¼å¼
                        local_path = f"/static/videos/{dataset_id}/{sample_id}/{os.path.basename(video_path)}"
                        video_paths.append(local_path)
                
                # åˆ›å»ºæ ‡å‡†æ ·æœ¬
                sample = {
                    "id": sample_id,
                    "name": take['take_name'],
                    "type": "multiple_videos" if len(video_paths) > 1 else "single_video",
                    "video_paths": video_paths,
                    "video_path": video_paths[0] if video_paths else None,
                    "assigned_to": f"annotator_{(i % 4) + 1}",  # å¾ªç¯åˆ†é…æ ‡æ³¨è€…
                    "review_status": "æœªå®¡é˜…",
                    "created_at": datetime.now().isoformat(),
                    "egoexo4d_metadata": {
                        "take_uid": take.get('take_uid', ''),
                        "root_dir": take.get('root_dir', ''),
                        "best_exo": take.get('best_exo', '')
                    }
                }
                
                standard_dataset["samples"].append(sample)
                # print(f"ğŸ“¹ è½¬æ¢æ ·æœ¬: {take['take_name']} -> {sample_id} ({len(video_paths)} ä¸ªè§†é¢‘)")
        
        print(f"âœ… EgoExo4Dè½¬æ¢å®Œæˆï¼Œå…± {len(standard_dataset['samples'])} ä¸ªæ ·æœ¬")
        return standard_dataset
    
    def _generate_egoexo4d_sample_id(self, take_name: str) -> str:
        """ä¸ºEgoExo4Dæ ·æœ¬ç”Ÿæˆå¹²å‡€çš„ID"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿
        clean_id = ''.join(c for c in take_name if c.isalnum() or c == '_')
        # ç¡®ä¿IDä¸ä¸ºç©º
        if not clean_id:
            clean_id = f"sample_{hash(take_name) % 10000}"
        return clean_id
    
    def _create_sample_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•"""
        # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
        sample_dataset = {
            "id": "test_dataset",
            "name": "Test Dataset",
            "description": "A simple test dataset for video download functionality",
            "created_at": datetime.now().isoformat(),
            "samples": [
                {
                    "id": "test_single",
                    "name": "Test Single Video",
                    "type": "single_video",
                    "video_path": "/static/videos/test_dataset/test_single/video.mp4",
                    "assigned_to": "annotator_1",
                    "review_status": "æœªå®¡é˜…",
                    "created_at": datetime.now().isoformat()
                },
                {
                    "id": "test_multi",
                    "name": "Test Multi Video",
                    "type": "multiple_videos",
                    "video_paths": [
                        "/static/videos/test_dataset/test_multi/video1.mp4",
                        "/static/videos/test_dataset/test_multi/video2.mp4"
                    ],
                    "assigned_to": "annotator_1",
                    "review_status": "æœªå®¡é˜…",
                    "created_at": datetime.now().isoformat()
                },
                {
                    "id": "test_youtube",
                    "name": "Test YouTube Video",
                    "type": "youtube",
                    "youtube_url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
                    "assigned_to": "annotator_1",
                    "review_status": "æœªå®¡é˜…",
                    "created_at": datetime.now().isoformat()
                }
            ]
        }
        
        # åˆ›å»ºç¤ºä¾‹ç‰‡æ®µæ•°æ®
        sample_segments = {
            "segments": [
                {
                    "id": "test_segment_1",
                    "sample_id": "test_single",
                    "video_path": "/static/videos/test_dataset/test_single/video.mp4",
                    "start_time": 0.0,
                    "end_time": 10.0,
                    "status": "å¾…æŠ‰æ‹©",
                    "created_at": datetime.now().isoformat()
                },
                {
                    "id": "test_segment_2",
                    "sample_id": "test_multi",
                    "video_paths": [
                        "/static/videos/test_dataset/test_multi/video1.mp4",
                        "/static/videos/test_dataset/test_multi/video2.mp4"
                    ],
                    "start_time": 0.0,
                    "end_time": 15.0,
                    "status": "å¾…æŠ‰æ‹©",
                    "created_at": datetime.now().isoformat()
                }
            ]
        }
        
        # ä¿å­˜ç¤ºä¾‹æ•°æ®
        with open(os.path.join(self.data_dir, "test_dataset.json"), 'w', encoding='utf-8') as f:
            json.dump(sample_dataset, f, ensure_ascii=False, indent=2)
        
        with open(os.path.join(self.data_dir, "test_dataset_segments.json"), 'w', encoding='utf-8') as f:
            json.dump(sample_segments, f, ensure_ascii=False, indent=2)
        
        # é‡æ–°åŠ è½½æ•°æ®
        self._load_datasets()
    
    def get_datasets_for_annotator(self, annotator: str) -> List[Dict]:
        """è·å–æŒ‡å®šæ ‡æ³¨è€…çš„æ•°æ®é›†"""
        if not annotator:
            return []
        
        result = []
        for dataset_id, dataset in self.datasets.items():
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é…ç»™è¯¥æ ‡æ³¨è€…çš„æ ·æœ¬
            has_assigned_samples = any(
                sample.get('assigned_to') == annotator 
                for sample in dataset.get('samples', [])
            )
            
            if has_assigned_samples:
                result.append({
                    'id': dataset_id,
                    'name': dataset.get('name', 'Unknown'),
                    'description': dataset.get('description', ''),
                    'sample_count': len(dataset.get('samples', [])),
                    'assigned_sample_count': len([
                        s for s in dataset.get('samples', [])
                        if s.get('assigned_to') == annotator
                    ])
                })
        
        return result
    
    def get_samples_for_dataset(self, dataset_id: str, annotator: str) -> List[Dict]:
        """è·å–æŒ‡å®šæ•°æ®é›†çš„æ ·æœ¬åˆ—è¡¨ï¼ŒæŒ‰å®¡é˜…çŠ¶æ€æ’åº"""
        if dataset_id not in self.datasets:
            return []
        
        dataset = self.datasets[dataset_id]
        samples = dataset.get('samples', [])
        
        # è¿‡æ»¤æŒ‡å®šæ ‡æ³¨è€…çš„æ ·æœ¬
        if annotator:
            samples = [s for s in samples if s.get('assigned_to') == annotator]
        
        # æŒ‰å®¡é˜…çŠ¶æ€æ’åºï¼šå®¡é˜…ä¸­ -> æœªå®¡é˜… -> å·²å®¡é˜…
        status_order = {'å®¡é˜…ä¸­': 0, 'æœªå®¡é˜…': 1, 'å·²å®¡é˜…': 2}
        samples.sort(key=lambda x: status_order.get(x.get('review_status', 'æœªå®¡é˜…'), 1))
        
        return samples
    
    def get_segments_for_dataset(self, dataset_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šæ•°æ®é›†çš„ç‰‡æ®µåˆ—è¡¨"""
        if dataset_id not in self.segments:
            return []
        
        segments = self.segments[dataset_id].get('segments', [])
        
        # æŒ‰çŠ¶æ€æ’åºï¼šå¾…æŠ‰æ‹© -> é€‰ç”¨ -> å¼ƒç”¨
        status_order = {'å¾…æŠ‰æ‹©': 0, 'é€‰ç”¨': 1, 'å¼ƒç”¨': 2}
        segments.sort(key=lambda x: status_order.get(x.get('status', 'å¾…æŠ‰æ‹©'), 0))
        
        return segments
    
    def get_segments_for_sample(self, sample_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šæ ·æœ¬çš„ç‰‡æ®µåˆ—è¡¨"""
        result = []
        for dataset_segments in self.segments.values():
            sample_segments = [
                s for s in dataset_segments.get('segments', [])
                if s.get('sample_id') == sample_id
            ]
            result.extend(sample_segments)
        
        # æŒ‰çŠ¶æ€æ’åº
        status_order = {'å¾…æŠ‰æ‹©': 0, 'é€‰ç”¨': 1, 'å¼ƒç”¨': 2}
        result.sort(key=lambda x: status_order.get(x.get('status', 'å¾…æŠ‰æ‹©'), 0))
        
        return result
    
    def create_segment(self, segment_data: Dict) -> bool:
        """åˆ›å»ºæ–°ç‰‡æ®µ"""
        try:
            sample_id = segment_data.get('sample_id')
            if not sample_id:
                return False
            
            # æ‰¾åˆ°å¯¹åº”çš„æ•°æ®é›†
            dataset_id = None
            for ds_id, dataset in self.datasets.items():
                if any(s.get('id') == sample_id for s in dataset.get('samples', [])):
                    dataset_id = ds_id
                    break
            
            if not dataset_id:
                return False
            
            # æ·»åŠ åˆ›å»ºæ—¶é—´
            segment_data['created_at'] = datetime.now().isoformat()
            
            # ç¡®ä¿æ•°æ®é›†æœ‰ç‰‡æ®µæ•°æ®ç»“æ„
            if dataset_id not in self.segments:
                self.segments[dataset_id] = {'segments': []}
            
            # æ·»åŠ æ–°ç‰‡æ®µ
            self.segments[dataset_id]['segments'].append(segment_data)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            filepath = os.path.join(self.data_dir, f"{dataset_id}_segments.json")
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.segments[dataset_id], f, ensure_ascii=False, indent=2)
            
            return True
        except Exception as e:
            print(f"Error creating segment: {e}")
            return False
    
    def update_segment(self, segment_id: str, update_data: Dict) -> bool:
        """æ›´æ–°ç‰‡æ®µä¿¡æ¯ï¼ˆçŠ¶æ€ã€æ—¶é—´ç­‰ï¼‰"""
        try:
            for dataset_id, dataset_segments in self.segments.items():
                for segment in dataset_segments.get('segments', []):
                    if segment.get('id') == segment_id:
                        # æ›´æ–°çŠ¶æ€
                        if 'status' in update_data:
                            segment['status'] = update_data['status']
                        # æ›´æ–°æ—¶é—´
                        if 'start_time' in update_data:
                            segment['start_time'] = update_data['start_time']
                        if 'end_time' in update_data:
                            segment['end_time'] = update_data['end_time']
                        
                        # ä¿å­˜åˆ°æ–‡ä»¶
                        filepath = os.path.join(self.data_dir, f"{dataset_id}_segments.json")
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(dataset_segments, f, ensure_ascii=False, indent=2)
                        return True
            return False
        except Exception as e:
            print(f"Error updating segment: {e}")
            return False
    
    def update_segment_status(self, segment_id: str, status: str) -> bool:
        """æ›´æ–°ç‰‡æ®µçŠ¶æ€ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        return self.update_segment(segment_id, {'status': status})
    
    def remove_rejected_segments(self, dataset_id: str) -> bool:
        """åˆ é™¤æ‰€æœ‰å¼ƒç”¨çš„ç‰‡æ®µ"""
        try:
            if dataset_id not in self.segments:
                return False
            
            dataset_segments = self.segments[dataset_id]
            # è¿‡æ»¤æ‰å¼ƒç”¨çš„ç‰‡æ®µ
            dataset_segments['segments'] = [
                s for s in dataset_segments.get('segments', [])
                if s.get('status') != 'å¼ƒç”¨'
            ]
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            filepath = os.path.join(self.data_dir, f"{dataset_id}_segments.json")
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(dataset_segments, f, ensure_ascii=False, indent=2)
            
            return True
        except Exception as e:
            print(f"Error removing rejected segments: {e}")
            return False
    
    def delete_segment(self, segment_id: str) -> bool:
        """åˆ é™¤æŒ‡å®šç‰‡æ®µ"""
        try:
            for dataset_id, dataset_segments in self.segments.items():
                for i, segment in enumerate(dataset_segments.get('segments', [])):
                    if segment.get('id') == segment_id:
                        # åˆ é™¤ç‰‡æ®µ
                        dataset_segments['segments'].pop(i)
                        
                        # ä¿å­˜åˆ°æ–‡ä»¶
                        filepath = os.path.join(self.data_dir, f"{dataset_id}_segments.json")
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(dataset_segments, f, ensure_ascii=False, indent=2)
                        return True
            return False
        except Exception as e:
            print(f"Error deleting segment: {e}")
            return False
    
    def mark_sample_reviewed(self, sample_id: str) -> bool:
        """æ ‡è®°æ ·æœ¬ä¸ºå·²å®¡é˜…"""
        try:
            for dataset_id, dataset in self.datasets.items():
                for sample in dataset.get('samples', []):
                    if sample.get('id') == sample_id:
                        # æ›´æ–°å®¡é˜…çŠ¶æ€
                        sample['review_status'] = 'å·²å®¡é˜…'
                        
                        # ä¿å­˜åˆ°æ–‡ä»¶
                        filepath = os.path.join(self.data_dir, f"{dataset_id}.json")
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(dataset, f, ensure_ascii=False, indent=2)
                        return True
            return False
        except Exception as e:
            print(f"Error marking sample as reviewed: {e}")
            return False
    
    def mark_sample_unreviewed(self, sample_id: str) -> bool:
        """æ ‡è®°æ ·æœ¬ä¸ºæœªå®¡é˜…"""
        try:
            for dataset_id, dataset in self.datasets.items():
                for sample in dataset.get('samples', []):
                    if sample.get('id') == sample_id:
                        # æ›´æ–°å®¡é˜…çŠ¶æ€
                        sample['review_status'] = 'æœªå®¡é˜…'
                        
                        # ä¿å­˜åˆ°æ–‡ä»¶
                        filepath = os.path.join(self.data_dir, f"{dataset_id}.json")
                        with open(filepath, 'w', encoding='utf-8') as f:
                            json.dump(dataset, f, ensure_ascii=False, indent=2)
                        return True
            return False
        except Exception as e:
            print(f"Error marking sample as unreviewed: {e}")
            return False
